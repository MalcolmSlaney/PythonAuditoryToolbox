#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Nov  1 13:57:56 2023
"""
import math
from typing import List, Optional
import torch
from torch import nn
from torchaudio.functional import lfilter


class ErbFilterBank(nn.Module):
    r"""Applies an Auditory Filterbank to data of dimension of `(..., time)` as
    described in the 'Auditory Toolbox - An Efficient Implementation of the
    Patterson-Holdsworth Auditory Filter Bank' by Malcolm Slaney, available on:
    <https://engineering.purdue.edu/~malcolm/interval/1998-010/AuditoryToolboxTechReport.pdf>

    Args:
        num_channels: Number of filters in the filterbank.
            Default: 64
        lowest_frequency: Lowest centre frequency in the filterbank.
            Default: 100
        sampling_rate: Sampling rate of input.
            Default: 16000
        dtype: (Optional) Cast coefficients to dtype after instantiation.
            Default: None

    .. note::
        The implementation does not attempt to account for filtering delays.
        Note also that uniform temporal sampling is assumed and that the data
        are not mean-centered or zero-padded prior to filtering.

    Examples:
        >>> m = ErbFilterBank(sampling_rate=16000)
        >>> m = m.to(device=torch.device("cpu"), dtype=torch.float32)
        >>> assert m.sos.dtype==torch.float32, "This cannot be true"
        >>> input = torch.zeros(1,512,dtype=torch.float32)
        >>> input[0,0 ] = 1.
        >>> output = m(input)

    Attributes:
        fcoefs: Filter coefficients generated by make_erb_filters
        sos: Coefficients used for subsequent filtering.
    """
    __constants__ = ["sampling_rate", "num_channels", "lowest_frequency"]

    def __init__(
            self,
            sampling_rate: int = 16000,
            num_channels: int = 64,
            lowest_frequency: int = 100,
            dtype: Optional[torch.dtype] = None,
    ) -> None:
        super().__init__()
        if sampling_rate <= 0:
            raise ValueError('Sampling rate cannoy be negative or zero')
        if lowest_frequency <= 0 or lowest_frequency >= sampling_rate/2:
            raise ValueError('Misspecified lowest frequency')

        self.sampling_rate = sampling_rate
        self.num_channels = num_channels
        self.lowest_frequency = lowest_frequency
        self.fcoefs = make_erb_filters(self.sampling_rate,
                                       self.num_channels,
                                       self.lowest_frequency)
        if dtype:
            sos = prepare_coefficients(self.fcoefs).to(dtype=dtype)
        else:
            sos = prepare_coefficients(self.fcoefs)

        self.register_buffer("sos", sos)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        r"""Pass audio through the set of filters.

        The code is directly adapted from: 'Auditory Toolbox - An Efficient
        Implementation of the Patterson-Holdsworth Auditory Filter Bank' by
        Malcolm Slaney.

        Parameters
        ----------
            x (Tensor): Tensor of audio of dimension (..., time).

        Returns
        -------
            Tensor: Output signal of dimension (..., num_channels, time).
        """
        if x.ndim < 2:
            raise ValueError('The input tensor should have size `(..., time)`')
        if x.shape[-1] <= 1:
            raise ValueError('The input tensor should have size `(..., time)`')
        new_dims = [1 for j in list(x.unsqueeze(-2).shape)]
        new_dims[-2] = self.num_channels

        y = lfilter(x.unsqueeze(-2).tile(new_dims),
                    self.sos[..., -1],
                    self.sos[..., 0],
                    clamp=False, batching=True)

        for j in range(3):
            y = lfilter(y,
                        self.sos[..., -1],
                        self.sos[..., j+1],
                        clamp=False, batching=True)

        return y


def erb_space(low_freq: float = 100,
              high_freq: float = 44100/4,
              n: int = 100) -> torch.Tensor:
    r"""Compute frequencies uniformly spaced on an erb scale.

    The code is directly adapted from: 'Auditory Toolbox - An Efficient
    Implementation of the Patterson-Holdsworth Auditory Filter Bank' by
    Malcolm Slaney.

    This function computes an array of N frequencies uniformly spaced
    between high_freq and low_freq on an erb scale. For a definition of erb,
    see Moore, B. C. J., and Glasberg, B. R. (1983)."Suggested formulae for
    calculating auditory-filter bandwidths and excitation patterns,"
    J. Acoust. Soc. Am. 74, 750-753.


    Parameters
    ----------
    low_freq : float, optional
        Lower limit. The default is 100.
    high_freq : float, optional
        Upper limit. The default is 44100/4.
    n : int, optional
        Number of frequencies. The default is 100.

    Returns
    -------
    cf_array : torch.Tensor
        Centre frequencies.

    """
    #  Change the following three parameters if you wish to use a different
    #  erb scale.  Must change in MakeerbCoeffs too.
    ear_q = 9.26449				# Glasberg and Moore Parameters
    min_bw = 24.7

    # All of the follow_freqing expressions are derived in Apple TR #35, "An
    # Efficient Implementation of the Patterson-Holdsworth Cochlear
    # Filter Bank."  See pages 33-34.
    cf_array = (-(ear_q*min_bw) + torch.exp(
        torch.arange(1, 1+n, dtype=torch.float64).unsqueeze(1) *
        (-math.log(high_freq + ear_q*min_bw) +
         math.log(low_freq + ear_q*min_bw))/n) * (high_freq + ear_q*min_bw))
    return cf_array


def make_erb_filters(fs: float, num_channels: int,
                     low_freq: float) -> List[torch.tensor]:
    r"""Compute filter coefficients for a bank of Gammatone filters.

    The code is directly adapted from: 'Auditory Toolbox - An Efficient
    Implementation of the Patterson-Holdsworth Auditory Filter Bank' by
    Malcolm Slaney.

    These filters were defined by Patterson and Holdworth for simulating the
    cochlea.

    The result is returned as a list of filter coefficients.  Each row
    of the filter arrays contains the coefficients for four second order
    filters.  The transfer function for these four filters share the same
    denominator (poles) but have different numerators (zeros).

    The filter bank contains "num_channels" channels that extend from
    half the sampling rate (fs) to "low_freq".  Alternatively, if the
    num_channels argument is a vector, then the values of this vector are taken
    to be the center frequency of each desired filter.


    Parameters
    ----------
    fs : float
        Sampling rate.
    num_channels : int or list of floats
        Number of channels or centre frequencies.
    low_freq : float
        Lower limit.

    Returns
    -------
    fcoefs : List[torch.tensor]
        Filter coefficients

    """
    t = 1/fs
    if isinstance(num_channels, int):
        cf = erb_space(low_freq, fs/2, num_channels)
    else:
        cf = num_channels

    erb = erb_bandwidth(cf)
    b = 1.019*2*math.pi*erb

    a11 = (-2 * t * torch.cos(2 * cf * math.pi * t) / torch.exp(b * t) + 2 *
           math.sqrt(3 + 2**1.5) * t * torch.sin(2 * cf * math.pi * t) /
           torch.exp(b * t)) / 2
    a12 = (-2 * t * torch.cos(2 * cf * math.pi * t) / torch.exp(b * t) - 2 *
           math.sqrt(3 + 2**1.5) * t * torch.sin(2 * cf * math.pi * t) /
           torch.exp(b * t)) / 2
    a13 = (-2 * t * torch.cos(2 * cf * math.pi * t) / torch.exp(b * t) + 2 *
           math.sqrt(3 - 2**1.5) * t * torch.sin(2 * cf * math.pi * t) /
           torch.exp(b * t)) / 2
    a14 = (-2 * t * torch.cos(2 * cf * math.pi * t) / torch.exp(b * t) - 2 *
           math.sqrt(3 - 2**1.5) * t * torch.sin(2 * cf * math.pi * t) /
           torch.exp(b * t)) / 2

    gain = torch.abs((-2*torch.exp(4*complex(0, 1)*cf*math.pi*t)*t +
                      2*torch.exp(-(b*t) + 2*complex(0, 1)*cf*math.pi*t)*t *
                      (torch.cos(2*cf*math.pi*t) - math.sqrt(3 - 2**(3/2)) *
                       torch.sin(2*cf*math.pi*t))) *
                     (-2*torch.exp(4*complex(0, 1)*cf*math.pi*t)*t +
                      2*torch.exp(-(b*t) + 2*complex(0, 1)*cf*math.pi*t)*t *
                      (torch.cos(2*cf*math.pi*t) + math.sqrt(3 - 2**(3/2)) *
                         torch.sin(2*cf*math.pi*t))) *
                     (-2*torch.exp(4*complex(0, 1)*cf*math.pi*t)*t +
                      2*torch.exp(-(b*t) + 2*complex(0, 1)*cf*math.pi*t)*t *
                      (torch.cos(2*cf*math.pi*t) -
                         math.sqrt(3 + 2**(3/2))*torch.sin(2*cf*math.pi*t))) *
                     (-2*torch.exp(4*complex(0, 1)*cf*math.pi*t)*t +
                      2*torch.exp(-(b*t) + 2*complex(0, 1)*cf*math.pi*t)*t *
                      (torch.cos(2*cf*math.pi*t) +
                       math.sqrt(3 + 2**(3/2))*torch.sin(2*cf*math.pi*t))) /
                     (-2 / torch.exp(2*b*t) -
                      2*torch.exp(4*complex(0, 1)*cf*math.pi*t) +
                      2*(1 + torch.exp(4*complex(0, 1)*cf*math.pi*t)) /
                      torch.exp(b*t))**4)

    fcoefs = [t * torch.ones(len(cf), 1, dtype=torch.float64),
              a11, a12, a13, a14,
              0 * torch.ones(len(cf), 1, dtype=torch.float64),
              1 * torch.ones(len(cf), 1, dtype=torch.float64),
              -2*torch.cos(2*cf*math.pi*t)/torch.exp(b*t),
              torch.exp(-2*b*t),
              gain]

    return fcoefs


def erb_bandwidth(cf: torch.tensor, ear_q: float = 9.26449,
                  min_bw: float = 24.7,
                  order: float = 1.) -> float:
    """Returns bandwidth of cochlear channel at a given frequency.

    Default parameters are defined according to Glasberg and Moore
    """

    erb = ((cf/ear_q)**order + min_bw**order)**(1/order)

    return erb


def prepare_coefficients(fcoefs: List[torch.tensor]) -> torch.tensor:
    r"""Reassemble filter coefficients to realize filters.

    The code is directly adapted from: 'Auditory Toolbox - An Efficient
    Implementation of the Patterson-Holdsworth Auditory Filter Bank' by
    Malcolm Slaney

    Parameters
    ----------
    fcoefs : List[torch.tensor]
        Coefficients prepared by make_erb_filters.

    Returns
    -------
    sos : torch.tensor
        Reassembled coefficients.

    """
    [a0, a11, a12, a13, a14, a2, b0, b1, b2, gain] = fcoefs
    n_chan = a0.shape[0]
    assert n_chan == a11.shape[0]
    assert n_chan == a12.shape[0]
    assert n_chan == a13.shape[0]
    assert n_chan == a14.shape[0]
    assert n_chan == b0.shape[0]
    assert n_chan == b1.shape[0]
    assert n_chan == gain.shape[0]

    sos = torch.cat([
        torch.cat([a0/gain,  a0,   a0, a0, b0], dim=1).unsqueeze(1),
        torch.cat([a11/gain, a12, a13, a14, b1], dim=1).unsqueeze(1),
        torch.cat([a2/gain,  a2,   a2, a2, b2], dim=1).unsqueeze(1),
    ], dim=1)

    return sos


def make_vowel(sample_len: int,
               pitch: float,
               sample_rate: float,
               f: str) -> torch.Tensor:
    r"""Synthesize an artificial vowel using formant filters.

    The code is directly adapted from MakeVowel by Malcolm Slaney

    Make a vowel with "sample_len" samples and the given pitch. The
    formant frequencies are f1, f2 & f3.  Some common vowels are
              Vowel       f1      f2      f3
               /a/        730    1090    2440
               /i/        270    2290    3010
               /u/        300     870    2240

    The pitch variable can either be a scalar indicating the actual
    pitch frequency, or an array of impulse locations. Using an
    array of impulses allows this routine to compute vowels with
    varying pitch.

    Alternatively, f1 can be replaced with one of the following strings
    'a', 'i', 'u' and the appropriate formant frequencies are
    automatically selected.

    Parameters
    ----------
    sample_len : int
        Length of sample.
    pitch : float
        Pitch frequency
    sample_rate : float
        Sample rate
    f : string or list

    Returns
    -------
    y : torch.Tensor
        Waveform

    """
    if isinstance(f, str):
        if f in ['a', '/a/']:
            f1, f2, f3 = (730, 1090, 2440)
        elif f in ['i', '/i/']:
            f1, f2, f3 = (270, 2290, 3010)
        elif f in ['u', '/u/']:
            f1, f2, f3 = (300, 870, 2240)
    elif isinstance(f, list) and len(f) == 3:
        f1, f2, f3 = f[0], f[1], f[2]
    else:
        f1, f2, f3 = 0., 0., 0.
    # GlottalPulses(pitch, fs, sample_len) - Generate a stream of
    # glottal pulses with the given pitch (in Hz) and sampling
    # frequency (sample_rate).  A vector of the requested length is
    # returned.
    y = torch.zeros(sample_len, dtype=torch.float64)
    if isinstance(pitch, (int, float)):
        points = torch.arange(0, sample_len, sample_rate /
                              pitch, dtype=torch.float64)
    else:
        points, __ = torch.sort(torch.as_tensor(pitch, dtype=torch.float64))
        points = points[points < sample_len-1]

    indices = torch.floor(points).to(torch.int16)

    #  Use a triangular approximation to an impulse function.  The important
    #  part is to keep the total amplitude the same.
    y[(indices).tolist()] = (indices+1)-points
    y[(indices+1).tolist()] = points-indices

    # GlottalFilter(x,fs) - Filter an impulse train and simulate the glottal
    # transfer function.  The sampling interval (sample_rate) is given in Hz.
    # The filtering performed by this function is two first-order filters
    # at 250Hz.
    y = glottal_filter(sample_rate, y)

    #  FormantFilter - Filter an input sequence to model one
    #    formant in a speech signal.  The formant frequency (in Hz) is given
    #    by f and the bandwidth of the formant is a constant 50Hz.  The
    #    sampling frequency in Hz is given by fs.
    if f1 > 0:
        y = formant_filter(f1, sample_rate, y)

    if f2 > 0:
        y = formant_filter(f2, sample_rate, y)

    if f3 > 0:
        y = formant_filter(f3, sample_rate, y)

    return y


def glottal_filter(sample_rate, x):
    r"""Glottal filter"""
    a = math.exp(-250*2*math.pi/sample_rate)
    return lfilter(x, torch.tensor([1, 0, -a*a], dtype=torch.float64),
                   torch.tensor([1, 0, 0], dtype=torch.float64), clamp=False)


def formant_filter(f, sample_rate, x):
    r"""Filter with a formant filter."""
    cft = f/sample_rate
    bw = 50
    q = f/bw
    rho = math.exp(-math.pi * cft / q)
    theta = 2 * math.pi * cft * math.sqrt(1-1/(4 * q*q))
    a2 = -2*rho*math.cos(theta)
    a3 = rho*rho
    a_coeffs = torch.tensor([1, a2, a3], dtype=torch.float64)
    b_coeffs = torch.tensor([1+a2+a3, 0, 0], dtype=torch.float64)
    return lfilter(x, a_coeffs, b_coeffs, clamp=False)


def fm_points(sample_len: int,
              freq: float,
              fm_freq: float = 6.,
              fm_amp: float = None,
              sampling_rate: float = 22050.) -> torch.Tensor:
    r"""Generate impulse train corresponding to a vibrato.

    The code is directly adapted from FMPoints by Malcolm Slaney

    Basic formula: phase angle = 2*pi*freq*t +
                            (fm_amp/fm_freq)*sin(2*pi*fm_freq*t)
        k-th zero crossing approximately at sample number
        (fs/freq)*(k - (fm_amp/(2*pi*fm_freq))*sin(2*pi*k*(fm_freq/freq)))

    Parameters
    ----------
    sample_len : int
        Number of samples.
    freq : float
        Pitch frequency (Hz)
    fm_freq : float
        Vibrato frequency (Hz)  (defaults to 6 Hz)
    fm_amp : float
        max change in pitch  (defaults to 5% of freq)
    sampling_rate : float
        Sample frequency (Hz)

    Returns
    -------
    y : torch.Tensor
        Waveform

    """

    if fm_amp is None:
        fm_amp = 0.05*freq

    kmax = int(math.floor(freq*(sample_len/sampling_rate)))
    points = torch.arange(kmax, dtype=torch.float64)
    y = 1 + (sampling_rate/freq)*(points-(
        fm_amp/(2*math.pi*fm_freq))*torch.sin(2*math.pi*(fm_freq/freq)*points))

    return y
